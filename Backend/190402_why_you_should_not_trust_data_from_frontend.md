# 为什么你不应该相信前端的数据

上次培训中，我们提到了，后端不应该相信前端的任何数据，可能有些同学不是很明白，这里给大家简单的介绍一下原因。

> PS：全文文字，需要耐心看

### 我们是如何确定数据是来自前端的

开始讲解之前，希望大家思考一下这个问题。在前后端的交互中，你是如何确定这个表单/数据是由前端发送的呢？你们可以先思考一下再往下看


如果大家了解http协议，应该知道基本上web上所有请求都是通过http协议进行的，一般我们用于传输数据的请求是用的POST请求，那么我们是如何区分请求是来自前端js的呢？再进一步，我们是如何确认请求是来自用户的操作呢？再更进一步，我们是如何确定请求是来自正常/可信用户的呢？\
提前给个我的答案：以我目前的知识水平来讲，几乎不可能。换句话说，我们无法通过对请求层面上的检查，就可以保证这个数据是可信的。

### 为什么我们无法确定请求的来源

#### Http协议的角度
网络上请求都是通过http协议来的，而伪造http请求的代价其实很低。HTTP请求头中有一些请求头用来判断发起请求的页面，例如Origin和Referer请求头。但是这两个请求头伪造的成本几乎为0。任意的后端语言都会有网络请求库，我们完全可以通过抓包，得到需要传递的参数和请求头，然后用任意语言（Python、Java、C++、golang）来发起这个post请求，这个post请求可以和前端发起的请求一模一样，所以服务端根本无从区分这个是用其他方式模拟的请求，还是正常请求。

#### 前端的角度
这时候可能有同学会说，可以通过前端字段加时间戳，然后让请求无法被重放。这个其实也不太现实，即便你的请求中包含了时间参数，但是如果你的前端代码仍然是明文的，那么我完全可以阅读完你的代码，然后按照前端的思路，直接调用你的前端代码函数（F12进入开发者后想怎么调用就怎么调用了），或者用后端实现一遍前端的代码，一样可以模拟出符合预设规定的请求。\
这时候也许又有同学会说，那把时间字段加密，然后再把前端代码混淆加密，让他看不出来这段代码是怎么做的。这个思路还算有一点点作用，但是现在的反混淆工具也不少，能力也不弱，只要还原到勉强能读的情况，你的代码逻辑还是会被摸清楚。即便你的混淆加密做的非常好，这些工具也没法解释出来的时候，仍有一招大杀器，就是启动无头浏览器（一种没有图形用户界面的浏览器。它可以被程序控制，从而自动执行某些任务），来模拟用户的点击和输入各种操作，最后在发请求前将数据包拦截并修改。这一切在实验完毕后都可以通过自动化方式进行，所以前端基本拦截不住这样的异常状况。

#### 本质上的问题
其实说到底，这个问题和爬虫与反爬虫很像。爬虫与反爬虫的对抗，其实本质就是在检测这个用户是不是机器人/自动化脚本，所以很多网站才会有验证码，那么相应的爬虫就有了验证码识别功能。后来为了对抗这种验证码识别，反爬虫一派又出了点击式验证码，运算式验证码（图片是公式，需要你计算结果）等等，这个就不在此文章讨论范围内了。我们只需要知道，目前两方并没有哪一方占据了完全的上风，反爬虫只能不断提高爬虫方的伪造成本，从而让他们放弃，但是如果有足够的利益，反爬虫是不会计较这些伪造成本的。

### 结论和建议
总的来说，后端要做的事情还是挺多的。我们需要尽可能降低垃圾数据/异常数据进数据库的可能性，要在一开始就检测出字段的问题。就比如手机号，即便前端用了正则检验了，但根据上文说的，有无数种简单的方式可以伪造请求绕过前端的限制，那么最终可能你的数据库里有的用户手机号就是111。然后如果这个人再坏一些，起了个脚本不停的给你的数据库刷这些内容，你的数据库里无效信息就达到了99%了。我们后端要做的，除了这种基本的校验之外，还要提高伪造和搞破坏的成本，比如校验完手机号和Origin、Referer，还得保证手机号唯一，避免他们用看似正常的一个手机号（符合正则表达式）来暴力塞满你的数据库。或者通过记录ip，来防止同个ip下的多次频繁操作（一秒中操作数超过n，可以大概率认定不是人类操作）。一般做到这里就差不多了，因为除非你得罪了某个程序员，不然一般人在没有利益的驱动下，是不会去购买代理池来应对你的封锁的，这方面也不再往下讲了，继续讲下去又是爬虫与反爬虫之间的对抗了，有兴趣的同学可以自行了解~

### 终

总之全文概括下来就一句话，千万不能相信前端传的任何数据，该做校验的，该做防刷的，还是得做，千万别偷懒~